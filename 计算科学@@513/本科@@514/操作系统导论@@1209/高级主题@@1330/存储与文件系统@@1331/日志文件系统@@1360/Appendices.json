{"hands_on_practices": [{"introduction": "日志文件系统的核心承诺是在系统意外崩溃后仍能保持文件系统的一致性。这个动手实践是一个思想实验，旨在帮助你具体理解这一承诺是如何实现的。通过逐步追踪文件操作，并分析在每个步骤后发生崩溃可能导致的磁盘状态，你将深入理解“原子性”在文件系统中的实际含义，以及日志提交点在确保原子更新中的关键作用 [@problem_id:3651370]。", "problem": "一个使用仅元数据日志（metadata-only journaling）并工作在有序模式（ordered mode）下的日志文件系统（FS），以默认的回写缓存（write-back caching）方式挂载。目录路径 `/dir` 初始存在，且不包含名为 `new` 或 `final` 的条目。目标名称 `final` 在以下序列执行前不存在。考虑以下操作序列：创建一个文件，写入数据，然后重命名它。在此过程中，应用程序在任何时候都没有发出文件同步（fsync）调用。可移植操作系统接口（POSIX）的重命名语义保证了原子性：从崩溃中恢复后，要么旧名称存在，要么新名称存在，但两者不会同时存在。日志层提供以下保证：事务将元数据记录在日志中；当事务的日志提交记录被持久化写入时，该事务即被提交；恢复过程会将在主文件系统结构中重放任何具有有效提交记录的事务；在有序模式下，任何因操作而变脏且通过元数据可访问的数据块，都会在相应元数据事务的提交记录被持久化写入之前，被刷写到主存储设备。\n\n系统执行以下步骤，用 $k$ 编号：\n- $k=1$：应用程序在 `/dir` 中创建文件 `new`，导致分配一个新的 inode $I$ 并插入一个目录条目 `new` $\\rightarrow$ $I$。这是由日志子系统作为事务 $T_1$ 跟踪的元数据。\n- $k=2$：应用程序向文件中写入 $2$ 个数据块 $B_1$ 和 $B_2$。这些数据被缓存在页面缓存（page cache）中并标记为脏（dirty）。这是数据，不会作为元数据被记入日志。\n- $k=3$：inode $I$ 的大小被更新为 $S$ 字节，以反映写入的数据。这是元数据，并且仍然是事务 $T_1$ 的一部分。\n- $k=4$：日志层写入 $T_1$ 的日志描述符，将元数据更改（例如，目录条目和 inode 更改）暂存到日志区域，并安排在任何提交之前对 $B_1$ 和 $B_2$ 进行有序回写。\n- $k=5$：在为 $B_1$ 和 $B_2$ 发出有序数据写入后，日志层将 $T_1$ 的提交记录写入日志。此时，如果发生崩溃，恢复过程将重放 $T_1$ 的元数据。\n- $k=6$：检查点（checkpointing）进程将 $T_1$ 的元数据从日志复制到主文件系统结构。然后，$T_1$ 的日志空间可能会被回收。\n- $k=7$：应用程序在 `/dir` 内将文件从 `new` 重命名为 `final`。重命名操作被记录为事务 $T_2$（仅元数据），其日志描述符和提交记录在此步骤中被写入（有序模式下不需要为 $T_2$ 进行额外的数据刷写，因为没有新的数据变得可访问）。\n- $k=8$：检查点进程将 $T_2$ 的元数据从日志复制到主文件系统结构。\n\n假设在恰好完成步骤 $k$ 之后（即在步骤 $k$ 和 $k+1$ 之间）发生电源故障。考虑以下可能的恢复后磁盘状态：\n- $\\mathrm{S1}$：在 `/dir` 中既不存在 `new` 也不存在 `final`。不存在为 $I$ 分配的元数据。数据块 $B_1$ 和 $B_2$ 可能已经被物理写入，也可能没有，但它们无法通过任何文件访问（并且任何物理写入，如果存在，都是无关紧要的，因为相应的元数据没有被提交）。\n- $\\mathrm{S2}$：条目 `new` 存在于 `/dir` 中，指向大小为 $S$ 的 inode $I$。数据块 $B_1$ 和 $B_2$ 位于磁盘上，并可通过 $I$ 访问。条目 `final` 不存在。\n- $\\mathrm{S3}$：条目 `final` 存在于 `/dir` 中，指向大小为 $S$ 的 inode $I$。数据块 $B_1$ 和 $B_2$ 位于磁盘上，并可通过 $I$ 访问。条目 `new` 不存在。\n- $\\mathrm{S4}$：`new` 和 `final` 同时存在于 `/dir` 中，指向同一个 inode $I$。\n\n哪个选项正确地描述了每个故障点 $k$ 可能的恢复后磁盘状态集合？\n\nA. 对于 $k \\in \\{1,2,3,4\\}$，只有 $\\mathrm{S1}$；对于 $k \\in \\{5,6\\}$，只有 $\\mathrm{S2}$；对于 $k \\in \\{7,8\\}$，只有 $\\mathrm{S3}$。\n\nB. 对于 $k \\in \\{1,2,3\\}$，只有 $\\mathrm{S1}$；对于 $k=4$，可能是 $\\mathrm{S1}$ 或 $\\mathrm{S2}$；对于 $k \\in \\{5,6\\}$，只有 $\\mathrm{S2}$；对于 $k \\in \\{7,8\\}$，只有 $\\mathrm{S3}$。\n\nC. 对于 $k \\in \\{1,2,3,4\\}$，只有 $\\mathrm{S1}$；对于 $k \\in \\{5,6\\}$，只有 $\\mathrm{S2}$；对于 $k=7$，可能是 $\\mathrm{S2}$、$\\mathrm{S3}$ 或 $\\mathrm{S4}$；对于 $k=8$，只有 $\\mathrm{S3}$。\n\nD. 对于 $k \\in \\{1,2,3,4\\}$，只有 $\\mathrm{S1}$；对于 $k=5$，可能是 $\\mathrm{S1}$ 或 $\\mathrm{S2}$，取决于 $B_1$ 和 $B_2$ 是否已到达磁盘；对于 $k=6$，只有 $\\mathrm{S2}$；对于 $k \\in \\{7,8\\}$，只有 $\\mathrm{S3}$。", "solution": "我们从一个在有序模式下运行的仅元数据日志文件系统（FS）的核心定义和保证开始：\n\n- 日志事务将元数据更改记录到一个单独的日志区域。当一个事务的日志提交记录被持久化写入时，该事务被视为已提交。在崩溃恢复期间，任何具有有效提交记录的事务都会被重放到主文件系统结构中，以确保元数据的原子性。\n- 在有序模式下，任何因事务而变得可访问的脏数据块（例如，由 inode 的大小和块映射引用的新分配的文件内容）都会在相应事务的提交记录写入之前被刷写到主存储设备。因此，如果元数据提交使得文件及其大小可见，那么该元数据引用的数据块在此之前已经刷写到主存储（受提交时硬件持久性限制的约束）。\n- 可移植操作系统接口（POSIX）的重命名语义保证了原子性：恢复后，要么旧名称存在，要么新名称存在，但两者不会同时存在。\n- 检查点（Checkpointing）将已提交的元数据从日志复制到主文件系统结构；然而，即使在崩溃时检查点尚未完成，仅凭提交记录的存在就足以让恢复过程使事务的元数据可见。\n\n我们现在按 $k$ 分析该序列：\n\n- 对于 $k \\in \\{1,2,3\\}$：尚未写入 $T_1$ 的提交记录。尽管在 $k=4$ 时元数据可能被暂存到日志中，但在 $k=4$ 之前甚至连日志描述符都没有。因此，在 $k \\in \\{1,2,3\\}$ 之后发生崩溃不会产生任何已提交的元数据。恢复过程不会创建该文件，因此磁盘上的状态是 $\\mathrm{S1}$。\n\n- 对于 $k=4$：$T_1$ 的日志描述符存在，但还没有提交记录。没有提交记录的事务会被恢复过程忽略。主区域的元数据更新尚未进行检查点，日志中未提交的元数据也不会被重放。因此，在 $k=4$ 处崩溃后，系统将产生 $\\mathrm{S1}$ 状态。\n\n- 对于 $k=5$：$T_1$ 的提交记录已被持久化写入。根据有序模式的规定，$T_1$ 使其可访问的数据块 $B_1$ 和 $B_2$ 在提交记录写入之前已经被刷写。因此，恢复后，文件以 `new` 的名称存在，其 inode 为 $I$，大小为 $S$，并且 $B_1,B_2$ 在磁盘上且可访问。磁盘上的状态是 $\\mathrm{S2}$。\n\n- 对于 $k=6$：将 $T_1$ 的元数据进行检查点写入主区域，与通过一个已提交但未检查点的事务进行恢复相比，并不会改变外部可见的状态。在 $k=6$ 处崩溃后，恢复过程会发现 $T_1$ 已提交，并在需要时重放它；磁盘上的状态仍然是 $\\mathrm{S2}$。\n\n- 对于 $k=7$：重命名操作被记录为 $T_2$。在有序模式下，$T_2$ 不需要额外的数据刷写，因为重命名只影响元数据（目录条目和可能的链接计数）。$T_2$ 的提交记录在 $k=7$ 时写入，这确保了恢复时的原子重命名语义。崩溃后，只有新名称存在，旧名称不存在。因此，磁盘上的状态是 $\\mathrm{S3}$。\n\n- 对于 $k=8$：对 $T_2$ 进行检查点操作，并不会改变外部可见的状态，其效果与通过一个已提交的 $T_2$ 进行恢复所能提供的状态相同。崩溃后，恢复过程将产生 $\\mathrm{S3}$ 状态。\n\n根据此分析，正确的描述是：\n- $k \\in \\{1,2,3,4\\} \\Rightarrow \\mathrm{S1}$,\n- $k \\in \\{5,6\\} \\Rightarrow \\mathrm{S2}$,\n- $k \\in \\{7,8\\} \\Rightarrow \\mathrm{S3}$。\n\n我们现在评估各个选项：\n\n选项 A：状态分配为：对于 $k \\in \\{1,2,3,4\\}$ 为 $\\mathrm{S1}$；对于 $k \\in \\{5,6\\}$ 为 $\\mathrm{S2}$；对于 $k \\in \\{7,8\\}$ 为 $\\mathrm{S3}$。这与推导相符。结论 — 正确。\n\n选项 B：声称在 $k=4$ 时，可能出现 $\\mathrm{S1}$ 或 $\\mathrm{S2}$。这与日志系统恢复时需要提交记录的要求相矛盾；仅有日志描述符（没有提交记录）时，恢复过程会忽略该事务，因此在 $k=4$ 时不可能出现 $\\mathrm{S2}$。结论 — 错误。\n\n选项 C：声称在 $k=7$ 时，可能出现 $\\mathrm{S2}$、$\\mathrm{S3}$ 或 $\\mathrm{S4}$。实际上，在 $k=7$ 时写入了 $T_2$ 的提交记录，恢复过程必须产生重命名后的状态 $\\mathrm{S3}$；只有在没有 $T_2$ 提交记录的情况下才会出现 $\\mathrm{S2}$。此外，$\\mathrm{S4}$ 违反了 POSIX 的原子重命名语义和日志系统的元数据原子性。结论 — 错误。\n\n选项 D：声称在 $k=5$ 时，可能出现 $\\mathrm{S1}$ 或 $\\mathrm{S2}$，这取决于 $B_1$ 和 $B_2$ 是否已到达磁盘。在有序模式下，$T_1$ 的提交记录只有在 $T_1$ 使其可访问的数据块被刷写后才会被写入；因此，如果提交记录存在，恢复过程将产生 $\\mathrm{S2}$，而不是 $\\mathrm{S1}$。已提交元数据的存在保证了文件的存在，而有序回写确保了数据在提交前已到达磁盘。结论 — 错误。\n\n因此，正确选项是 A。", "answer": "$$\\boxed{A}$$", "id": "3651370"}, {"introduction": "当然，日志系统提供的这种安全性并非没有代价，它通常会带来额外的写操作，这种现象被称为“写放大”（Write Amplification）。这个练习将引导你从概念理解转向定量分析。通过为两种常见的日志模式——数据日志模式和元数据日志模式——推导写放大公式，你将学会如何量化不同保护级别与性能之间的权衡 [@problem_id:3651432]。", "problem": "考虑一个执行事务以提供原子更新的日志文件系统。每个事务恰好包含 $n$ 个应用程序数据块和 $m$ 个文件系统元数据块，这些是应用程序逻辑上更新的内容。所有块的大小相等，任何对单个块的物理写入都计为写入总数的一个单位。该系统可以在以下两种模式之一运行：\n\n- 数据日志模式（Data journaling mode）：应用程序数据和元数据更新都首先被写入日志，然后传播到它们的原始位置。\n- 仅元数据日志模式（Metadata-only journaling mode）：只有元数据更新被写入日志；应用程序数据以保持一致性的顺序直接写入其原始位置。\n\n对于这两种模式，假设存在以下操作细节：\n- 每个事务都会向日志追加一个大小为 $1$ 个块的日志提交记录，以标记原子性完成。\n- 提交后，所有写入日志且代表实际文件系统状态的块（即应用程序数据和元数据更新）随后会被检查点（checkpoint）到它们的原始位置。\n- 忽略后台垃圾回收、检查点之外的日志清理、压缩、部分块更新、设备内复制以及任何更低级别的设备效应；仅计算主机可见的物理块写入。\n- 每个事务的逻辑数据负载为 $n$ 个块（应用程序的数据写入），文件系统元数据更新总计为每个事务 $m$ 个块。\n- 日志提交记录不对应于逻辑应用程序数据块，也不会被检查点到原始位置。\n\n从写放大（write amplification）$WA$ 的定义（即物理写入的总块数与逻辑写入的应用程序数据块数之比）出发，推导两种模式下作为 $n$ 和 $m$ 的函数的写放大的封闭形式表达式。将您的最终答案表示为两个最简形式的解析表达式，分别对应数据日志模式和仅元数据日志模式。请将最终答案以行矩阵的形式提供，其中第一个条目是数据日志模式的表达式，第二个条目是仅元数据日志模式的表达式。无需四舍五入，由于 $WA$ 是无量纲的，因此不涉及物理单位。", "solution": "该问题是有效的，因为它科学地基于操作系统和文件系统设计的原理，问题陈述清晰且提供了所有必要信息，并以客观、正式的语言表述。我们可以开始推导。\n\n写放大，记为 $WA$，定义为物理写入的总块数与逻辑写入的应用程序数据块数之比。\n$$WA = \\frac{\\text{Total Physical Blocks Written}}{\\text{Logical Application Data Blocks Written}}$$\n根据问题陈述，每个事务的逻辑数据负载包含 $n$ 个块。因此，对于两种日志模式，$WA$ 表达式的分母都是 $n$。\n$$\\text{Logical Application Data Blocks Written} = n$$\n\n我们现在将为每种模式推导物理写入的总块数。\n\n**1. 数据日志模式**\n\n在数据日志模式下，应用程序数据和元数据首先被写入日志，然后被检查点到存储设备上的最终位置（它们的“原始”位置）。每个事务的物理写入总数是写入日志的量和检查点期间写入的量之和。\n\n首先，我们计算写入日志的块数：\n- $n$ 个应用程序数据块被写入日志。\n- $m$ 个文件系统元数据块被写入日志。\n- 一个大小为 $1$ 个块的日志提交记录被写入日志，以标记事务完成。\n\n写入日志的总块数是这些部分的总和：\n$$W_{\\text{journal, data}} = n + m + 1$$\n\n接下来，我们计算检查点阶段写入的块数。检查点过程涉及将日志中的数据和元数据写入它们的原始位置。\n- $n$ 个应用程序数据块从日志写入其原始位置。\n- $m$ 个元数据块从日志写入其原始位置。\n- 日志提交记录是日志的内部结构，不会被检查点到原始位置。\n\n在检查点期间写入的总块数是：\n$$W_{\\text{checkpoint, data}} = n + m$$\n\n在数据日志模式下，每个事务的物理写入总块数 $W_{\\text{total, data}}$ 是日志写入和检查点写入的总和：\n$$W_{\\text{total, data}} = W_{\\text{journal, data}} + W_{\\text{checkpoint, data}} = (n + m + 1) + (n + m) = 2n + 2m + 1$$\n\n现在，我们可以计算数据日志模式的写放大 $WA_{\\text{data}}$：\n$$WA_{\\text{data}} = \\frac{W_{\\text{total, data}}}{n} = \\frac{2n + 2m + 1}{n}$$\n这可以简化为：\n$$WA_{\\text{data}} = \\frac{2n}{n} + \\frac{2m + 1}{n} = 2 + \\frac{2m + 1}{n}$$\n\n**2. 仅元数据日志模式**\n\n在仅元数据日志模式下，只有元数据更新被写入日志。应用程序数据被直接写入其原始位置。这改变了物理写入的计算方式。\n\n首先，我们统计作为事务执行一部分发生的初始写入：\n- $n$ 个应用程序数据块被直接写入其原始位置。\n- $m$ 个文件系统元数据块被写入日志。\n- 一个大小为 $1$ 个块的日志提交记录被写入日志。\n\n初始物理写入的总数是：\n$$W_{\\text{initial, meta}} = n + m + 1$$\n\n接下来，我们计算检查点阶段写入的块数。在这种模式下，只有元数据驻留在日志中。\n- $m$ 个元数据块从日志写入其原始位置。\n- 应用程序数据已经被写入其原始位置，因此不需要再为其进行写入。\n- 日志提交记录不被检查点。\n\n在检查点期间写入的总块数是：\n$$W_{\\text{checkpoint, meta}} = m$$\n\n在仅元数据日志模式下，每个事务的物理写入总块数 $W_{\\text{total, meta}}$ 是初始写入和检查点写入的总和：\n$$W_{\\text{total, meta}} = W_{\\text{initial, meta}} + W_{\\text{checkpoint, meta}} = (n + m + 1) + m = n + 2m + 1$$\n\n现在，我们可以计算仅元数据日志模式的写放大 $WA_{\\text{meta}}$：\n$$WA_{\\text{meta}} = \\frac{W_{\\text{total, meta}}}{n} = \\frac{n + 2m + 1}{n}$$\n这可以简化为：\n$$WA_{\\text{meta}} = \\frac{n}{n} + \\frac{2m + 1}{n} = 1 + \\frac{2m + 1}{n}$$\n\n问题要求最终答案是一个行矩阵，分别包含数据日志模式和仅元数据日志模式的表达式。", "answer": "$$\\boxed{\\begin{pmatrix} 2 + \\frac{2m+1}{n}  1 + \\frac{2m+1}{n} \\end{pmatrix}}$$", "id": "3651432"}, {"introduction": "理论模型与现实世界之间存在着差距，文件系统的逻辑保证依赖于底层硬件的正确协同。这个实践将你置于一个危险但极具启发性的真实场景中：当硬件被允许重新排序写操作时，文件系统的承诺是如何被打破的。通过设计一个实验来触发这种由写缓存导致的写乱序，你将理解为何像写屏障（write barriers）这样的底层机制对于确保端到端的数据完整性至关重要 [@problem_id:3651387]。", "problem": "一个使用元数据预写式日志（write-ahead logging）的日志文件系统（例如，一个采用 ordered 模式的类 ext4 设计）在正常情况下保证，日志中记录的元数据在其所依赖的数据块被持久化之前，不被视为持久的。这一保证依赖于使用写入屏障（write barriers）或缓存刷新（cache flushes）等机制来强制数据、日志元数据和日志提交记录之间的写入顺序。现在考虑一个存储设备，它带有一个易失性写缓存，该缓存可能会重排写入并在断电时丢失缓存数据。文件系统被显式挂载并禁用了屏障，因此文件系统不会发出强制单元访问（force-unit-access）或刷新（flush）命令来强制顺序。\n\n你的任务是确定哪一个实验既 (i) 最有可能导致日志的提交记录在相应的数据块之前到达稳定存储，又 (ii) 能正确预测突然断电及随后的日志恢复之后可观察到的磁盘不一致性。假设应用程序执行适合单个大小为 $4\\,\\text{KB}$ 块的小文件更新，系统缓冲缓存可以延迟数据写回，并且 rename 更新和目录条目作为元数据被记录到日志中。你可以假设崩溃恢复会重放日志中存在有效提交记录的任何事务。\n\n哪个选项最能同时满足这两个标准？\n\nA. 执行原子替换模式，但不显式同步数据文件：将新的 $4\\,\\text{KB}$ 负载写入一个临时文件，调用 rename 将其移动到目标路径上，然后对包含目录调用 fsync，但从不对临时文件的文件描述符调用 fsync 或 fdatasync。在目录 fsync 返回后立即切断电源。恢复后，目标文件名存在且具有预期的新大小，但其内容是旧版本或零，这揭示了日志提交已到达稳定存储，而数据块没有。\n\nB. 使用 O_DIRECT 打开目标文件并直接将新的 $4\\,\\text{KB}$ 负载写入文件，然后将一个占位符文件重命名到其上，不对任何文件或目录调用 fsync 或 fdatasync。立即切断电源。恢复后，目标文件完全丢失，这表明提交记录在数据之前到达了稳定存储。\n\nC. 执行原子替换模式，但在写入新的 $4\\,\\text{KB}$ 负载后立即对临时文件额外调用 fsync，然后调用 rename 并对包含目录调用 fsync。在目录 fsync 返回后立即切断电源。恢复后，目标文件名存在且具有预期的大小，但包含旧数据，这表明了提交先于数据。\n\nD. 在数据日志模式（data journaling mode）下挂载文件系统并禁用屏障。将新的 $4\\,\\text{KB}$ 负载写入文件，调用 rename，且不对任何文件或目录调用 fsync。立即切断电源。恢复后，如果数据丢失，日志校验和会阻止任何重放，因此文件不会以新名称出现，这证明了提交先于数据的情况不会发生。\n\nE. 使用原子替换模式，不调用任何 fsync，然后休眠 $t$ 秒（其中 $t$ 足够大），然后切断电源。恢复后，文件包含新数据，因为基于时间的延迟即使没有屏障也能保证顺序，所以提交先于数据的情况是不可观察的。", "solution": "用户希望我验证并解决一个与日志文件系统相关的问题。\n\n### 第一步：提取已知条件\n- **文件系统**：使用元数据预写式日志的日志文件系统。\n- **模式**：类似 `ext4` 的 ordered 模式设计。\n- **正常保证**：在正常情况下，日志中记录的元数据在其所依赖的数据块被持久化之前，不被视为持久的。\n- **强制机制**：通过写入屏障或缓存刷新来强制写入顺序。\n- **场景配置**：\n    - 存储设备具有易失性写缓存。\n    - 缓存可以重排写入。\n    - 缓存会在断电时丢失数据。\n    - 文件系统以禁用屏障的方式挂载。\n    - 文件系统不发出强制单元访问或刷新命令。\n- **任务**：确定哪个实验 (i) 最有可能导致日志的提交记录在相应的数据块之前到达稳定存储，以及 (ii) 正确预测断电和随后的日志恢复后可观察到的磁盘不一致性。\n- **假设**：\n    - 应用程序执行适合单个 $4\\,\\text{KB}$ 块的小文件更新。\n    - 系统缓冲缓存可以延迟数据写回。\n    - `rename` 更新和目录条目作为元数据被记录到日志中。\n    - 崩溃恢复会重放日志中找到有效提交记录的任何事务。\n\n### 第二步：使用已知条件进行验证\n问题陈述描述了一个现实但危险的现代操作系统和存储栈配置。\n- **科学上合理**：预写式日志、`ext4` 的 ordered 模式、易失性设备缓存、写入屏障（`FUA` 位、缓存刷新）、原子 `rename` 模式以及 `fsync` 系统调用等概念，都是计算机科学中（特别是在操作系统和文件系统设计领域）基础且描述准确的主题。该场景探讨了禁用写入屏障的众所周知的后果。其前提在事实上和科学上都是合理的。\n- **问题定义良好**：问题结构清晰。它定义了一个系统配置和一个故障模型（断电），并要求确定一个实验过程及其结果，该过程最能展示一种特定类型的故障（写入重排导致数据损坏）。目标明确无歧。\n- **客观性**：语言技术性强且精确。所有使用的术语在操作系统上下文中都有标准的、客观的含义。\n\n该问题并非不合理、不可形式化、不完整、不现实、定义不善、微不足道或无法验证。这个场景是一个经典的例子，用于教授 I/O 屏障在确保文件系统完整性方面的重要性。\n\n### 第三步：判断与行动\n问题陈述是有效的。我现在将着手推导解决方案。\n\n### 基于原理的推导\n这个问题的核心在于文件系统的日志模式保证、操作系统的 I/O 调度以及硬件存储设备的缓存行为之间的相互作用。\n\n$1$. **`ordered` 模式**：在像 `ext4` 这样以 `ordered` 模式运行的日志文件系统中，有一个特定的契约：数据块必须在引用它们的元数据被提交到日志*之前*写入其在磁盘上的最终位置。例如，创建新文件时，文件的内容数据必须先落盘，然后包含该文件 inode 和目录条目可见性的日志事务才能被提交。这可以防止在崩溃后出现元数据指向未初始化或垃圾数据块的情况。\n\n$2$. **写入屏障**：`ordered` 模式的保证是通过控制对物理存储介质的写入顺序来强制执行的。操作的逻辑顺序是：\n    a. 发出数据块的写入命令。\n    b. 发出写入屏障（例如，缓存刷新命令或设置了“强制单元访问”位的写入）。此命令指示磁盘驱动器在继续操作前，确保所有先前发出的写入都已提交到非易失性的稳定存储中。\n    c. 发出日志元数据和最终日志提交记录的写入命令。\n步骤 (b) 的屏障至关重要。它创建了一个同步点，确保数据持久化先于元数据提交的持久化。\n\n$3$. **禁用的屏障和易失性缓存**：问题指明屏障被禁用，且设备具有一个可以重排写入的易失性写缓存。这种配置打破了 `ordered` 模式的保证。文件系统仍然以正确的逻辑顺序（先数据，后日志提交）发出写入，但没有屏障，就没有强制执行机制。这两组写入——数据和日志提交——被发送到磁盘控制器，并可能停留在其易失性缓存中。磁盘控制器可以自由地以它认为最高效的任何顺序将缓存的块写入物理盘片（例如，为了最小化磁头寻道时间）。一个小的、顺序的写入，如单个日志提交块，通常比一个可能更大或更分散的数据写入更快地被持久化。\n\n$4$. **竞争条件与不一致性**：这造成了一个竞争条件。日志提交记录很可能（甚至是大概率）在依赖它的数据块仍处于易失性缓存中时就被写入稳定存储。如果此时发生电源故障，易失性缓存的内容将丢失。重启后，文件系统恢复过程会扫描日志。根据问题陈述，它会找到一个有效的提交记录并重放该事务。此事务会更新文件系统的元数据结构以指向新数据。然而，新数据本身在电源故障中丢失了。inode 中的块指针将指向磁盘上的一个块，该块包含陈旧数据（来自先前删除的文件）或零（如果该块是新分配的）。这导致了一种可观察到的磁盘不一致性：文件具有正确的名称和大小，但其内容已损坏。\n\n任务是找到最有可能引发并正确描述这种特定不一致性的实验。\n\n### 逐项分析\n\n**A. 执行原子替换模式，但不显式同步数据文件：将新的 $4\\,\\text{KB}$ 负载写入一个临时文件，调用 rename 将其移动到目标路径上，然后对包含目录调用 fsync，但从不对临时文件的文件描述符调用 fsync 或 fdatasync。在目录 fsync 返回后立即切断电源。恢复后，目标文件名存在且具有预期的新大小，但其内容是旧版本或零，这揭示了日志提交已到达稳定存储，而数据块没有。**\n\n-   **分析**：这个实验旨在触发上述确切的竞争条件。\n    -   `write()`：新的 $4\\,\\text{KB}$ 数据被写入操作系统页缓存，将该页标记为脏页。\n    -   `rename()`：这是一个元数据操作，被记录在一个日志事务中。在 `ordered` 模式下，文件系统知道这个元数据依赖于脏数据块，并为该数据块发出一个写操作。\n    -   `fsync(directory)`：此调用强制目录的元数据持久化。在日志文件系统中，这意味着强制提交包含 `rename()` 的日志事务。文件系统将发出数据块的写命令，随后是日志提交记录的写命令，但关键是*两者之间没有屏障*。\n    -   这就为设备重排写入创造了最高的可能性，即在数据之前持久化日志提交。\n    -   预测的结果也是正确的。恢复过程从日志中重放 `rename`。文件以新名称和正确的大小出现（来自日志中的 inode 元数据），但它所指向的数据块包含垃圾/陈旧数据，因为新数据的写入丢失了。\n-   **结论**：**正确**。此选项正确地识别了一个极有可能导致该问题的实验，并准确预测了由此产生的磁盘不一致性。\n\n**B. 使用 O_DIRECT 打开目标文件并直接将新的 $4\\,\\text{KB}$ 负载写入文件，然后将一个占位符文件重命名到其上，不对任何文件或目录调用 fsync 或 fdatasync。立即切断电源。恢复后，目标文件完全丢失，这表明提交记录在数据之前到达了稳定存储。**\n\n-   **分析**：\n    -   `O_DIRECT`：此标志绕过了操作系统页缓存，但它不绕过设备的易失性写缓存。写入被提交到设备，但当 `write()` 调用返回时，并不能保证它已在稳定存储上。\n    -   `无 fsync`：不对文件或目录执行 `fsync`，就没有触发器来强制 `rename` 的日志事务快速提交到磁盘。它将由一个后台内核进程在一段时间延迟后提交。“立即”切断电源使得提交记录甚至不太可能被*写入*到磁盘缓存中，更不用说被持久化了。\n    -   预测的结果“目标文件完全丢失”意味着 `rename` 事务未被重放，这表明提交记录没有到达稳定存储。这与展示“提交记录在数据之前到达稳定存储”的目标相矛盾。\n-   **结论**：**不正确**。这个实验触发竞争条件的可能性比 A 小，并且预测的结果没有展示所期望的故障模式。\n\n**C. 执行原子替换模式，但在写入新的 $4\\,\\text{KB}$ 负载后立即对临时文件额外调用 fsync，然后调用 rename 并对包含目录调用 fsync。在目录 fsync 返回后立即切断电源。恢复后，目标文件名存在且具有预期的大小，但包含旧数据，这表明了提交先于数据。**\n\n-   **分析**：这个实验包括了 `fsync(temporary file)`。这个系统调用的目的恰恰是强制文件的数据到达稳定存储。虽然禁用屏障削弱了 `fsync` 的保证，但该调用仍然明确指示系统优先写入该数据。这个操作主动地*阻止*了元数据在数据之前写入的竞争条件的发生。它使得期望的故障模式*更不可能*发生，而不是*更可能*。我们的目标是找到*最有可能*导致不一致性的实验。添加数据 `fsync` 是*防止*这种不一致性的标准编程实践。\n-   **结论**：**不正确**。这个实验设计不佳，无法实现所述目标，因为它包含了一个旨在防止其试图制造的问题的步骤。\n\n**D. 在数据日志模式（data journaling mode）下挂载文件系统并禁用屏障。将新的 $4\\,\\text{KB}$ 负载写入文件，调用 rename，且不对任何文件或目录调用 fsync。立即切断电源。恢复后，如果数据丢失，日志校验和会阻止任何重放，因此文件不会以新名称出现，这证明了提交先于数据的情况不会发生。**\n\n-   **分析**：此选项将日志模式更改为 `data=journal`。在此模式下，数据和元数据都写入日志。提交先于数据的竞争条件仍然可能发生（即，日志提交块在日志数据块之前写入磁盘）。然而，现代日志包含内部一致性检查，如校验和。如果恢复过程发现一个提交记录，但日志中相关的数​​据块丢失或损坏（因为它们从易失性缓存中丢失了），它将认为整个事务无效并丢弃它。结果是 `rename` 不会被重放，文件系统保持其先前的状态。这不会导致“可观察到的磁盘不一致性”（即文件数据损坏）；而是导致事务被安全地中止。该选项的结论“这证明了提交先于数据的情况不会发生”也不精确；物理上的写入重排可能发生，但文件系统的恢复逻辑稳健地处理了它，防止了数据损坏。\n-   **结论**：**不正确**。由于不同的日志模式及其相关的恢复保证，该实验不会产生目标不一致性（损坏的文件数据）。\n\n**E. 使用原子替换模式，不调用任何 fsync，然后休眠 $t$ 秒（其中 $t$ 足够大），然后切断电源。恢复后，文件包含新数据，因为基于时间的延迟即使没有屏障也能保证顺序，所以提交先于数据的情况是不可观察的。**\n\n-   **分析**：此选项的核心前提——即基于时间的延迟“即使没有屏障也能保证顺序”——是根本错误的。操作系统可能会在超时（$t$）后将脏页写回到磁盘，但这些写入会落在磁盘的易失性缓存中。在没有显式屏障或刷新命令的情况下，物理介质层面上没有持久性或顺序的保证。`sleep` 只会增加写入被发送到驱动器的概率，而不能保证它们以特定顺序被持久存储。因此，该实验不能可靠地产生任何特定结果，其推理基于对存储保证的错误理解。\n-   **结论**：**不正确**。其推理基于一个错误的科学前提。", "answer": "$$\\boxed{A}$$", "id": "3651387"}]}