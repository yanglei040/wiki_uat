{"hands_on_practices": [{"introduction": "理论知识告诉我们大页（Huge Pages）可以提升性能，但如何在实际中有效利用它们呢？仅仅在操作系统层面启用该功能往往是不够的，应用程序的内存分配策略也至关重要。本实践将通过一个思想实验，揭示不当的内存对齐是如何阻碍透明大页（Transparent Huge Pages, THP）机制发挥作用的，并引导你设计一个能满足对齐要求的内存分配器，从而显著减少地址翻译旁路缓冲（TLB）未命中次数，将理论优势转化为实际的性能增益 [@problem_id:3684889]。", "problem": "考虑一个实现了请求分页虚拟内存的系统，其基本页面大小为 $4\\,\\mathrm{KiB}$。操作系统启用了透明大页 (Transparent Huge Pages, THP)，当满足某些条件时，THP 可以将一组基本页面提升为大小为 $2\\,\\mathrm{MiB}$ 的大页。此处相关的主要条件是：大页区域必须是单个虚拟内存区域 (virtual memory area, VMA) 内的一个虚拟连续区域，起始虚拟地址必须与 $2\\,\\mathrm{MiB}$ 边界对齐，并且该 $2\\,\\mathrm{MiB}$ 区域内的所有基本页面都必须已填充且符合条件。快表 (Translation Lookaside Buffer, TLB) 用于缓存页表条目；当工作集远超 TLB 的覆盖范围且访问模式为流式扫描时，一个简单且常用的模型将每轮遍历工作集的 TLB 未命中次数视为与遍历的不同页面数量相等。Mebibyte (MiB) 表示 $2^{20}$ 字节，Kibibyte (KiB) 表示 $2^{10}$ 字节。\n\n一个程序使用默认分配器分配了 $N = 128$ 个数组，每个数组的大小为 $S = 1\\,\\mathrm{MiB}$。该分配器返回 16 字节对齐的堆块，并通过内核以 $4\\,\\mathrm{KiB}$ 的粒度使用独立的匿名映射来处理大额请求。程序会顺序访问这些数组，从头到尾扫描每个数组，然后继续扫描下一个数组，并重复此模式。该程序每轮的总扫描足迹为 $W = N \\times S = 128\\,\\mathrm{MiB}$。\n\n要求您从虚拟内存和 TLB 操作的基本原理出发，解释未对齐的分配如何阻止 THP 提升，并设计一个能为此工作负载启用 THP 的对齐感知分配器。在上述流式模型下，预测当从默认分配器切换到您设计的对齐感知分配器时，每轮 TLB 未命中次数的减少情况。\n\n请选择唯一最佳选项，该选项需正确陈述一个有效的分配器设计，并对在给定模型和假设下 TLB 未命中的减少量做出量化上正确的预测。\n\nA. 未对齐阻止了 THP，因为大页需要在单个 VMA 内有 $2\\,\\mathrm{MiB}$ 对齐的区域。设计一个分配器，通过系统调用（例如，请求 $4\\,\\mathrm{MiB}$）预留至少 $2\\,\\mathrm{MiB}$ 外加一些冗余空间，计算该预留空间内下一个 $2\\,\\mathrm{MiB}$ 对齐的基地址，通过取消映射来裁剪掉任何前导和尾随的冗余空间，然后从此 $2\\,\\mathrm{MiB}$ 对齐的超级块中为两个 $1\\,\\mathrm{MiB}$ 的数组进行子分配，并可选择使用建议性调用来倾向于使用 THP。对于 $W = 128\\,\\mathrm{MiB}$ 的顺序扫描，TLB 未命中次数从 $W / 4\\,\\mathrm{KiB} = 32768$ 下降到 $W / 2\\,\\mathrm{MiB} = 64$。\n\nB. 将分配对齐到 $64\\,\\mathrm{KiB}$ 对于 THP 来说是足够的，因为硬件可以映射子大页单元。确保 $64\\,\\mathrm{KiB}$ 对齐的 malloc 将 TLB 未命中次数从 $W / 4\\,\\mathrm{KiB} = 32768$ 减少到 $W / 64\\,\\mathrm{KiB} = 2048$。\n\nC. 使用标准的对齐分配调用将每个 $1\\,\\mathrm{MiB}$ 数组放置在 $2\\,\\mathrm{MiB}$ 边界上，即使每个数组都在其自己的映射中，也足以实现 THP 提升，因此 TLB 未命中次数从 $32768$ 减少到 $W / 1\\,\\mathrm{MiB} = 128$。\n\nD. 当内核的后台合并器运行时，THP 提升不受对齐影响，因此正确的设计是保持 malloc 不变；每轮的 TLB 未命中次数仍为 $32768$ 次，因为在这种情况下，是 TLB 容量而不是对齐决定了未命中次数。\n\nE. 为避免碎片化，通过建议性调用禁用 THP 并保留 $4\\,\\mathrm{KiB}$ 页面；这会改善局部性，并将 TLB 未命中次数减少到 $W / 8\\,\\mathrm{KiB} = 16384$，因为在没有大页的情况下实现了更大的查找粒度。", "solution": "分析分两部分进行：首先，计算使用默认分配器时的 TLB 未命中次数；其次，设计一个对齐感知的分配器并计算由此产生的 TLB 未命中次数。\n\n**第一部分：默认分配器分析**\n\n默认分配器通过独立的匿名映射来处理大额请求（例如为 $1\\,\\mathrm{MiB}$ 数组分配空间）。这意味着 $N=128$ 个数组中的每一个都可能驻留在其自己的虚拟内存区域 (Virtual Memory Area, VMA) 中。透明大页 (THP) 机制对于将一系列基本页面提升为大页有几项严格要求。如问题所述，这些要求包括：\n1.  该区域必须是单个 VMA 内的一个虚拟连续区域。\n2.  起始虚拟地址必须与 $2\\,\\mathrm{MiB}$ 边界对齐。\n3.  该 $2\\,\\mathrm{MiB}$ 区域内的所有基本页面都必须已填充。\n\n由于每个 $S=1\\,\\mathrm{MiB}$ 的数组位于独立的 VMA 中，因此不可能通过组合两个相邻数组来形成一个 $2\\,\\mathrm{MiB}$ 的大页，因为这会违反第一个条件。此外，默认分配器仅保证 16 字节对齐，这不满足要求 $2\\,\\mathrm{MiB}$ 对齐的第二个条件。因此，使用默认分配器时，THP 提升是不可能的。内存将使用 $4\\,\\mathrm{KiB}$ 的基本页面大小进行映射。\n\n工作集的总大小为 $W = N \\times S = 128 \\times 1\\,\\mathrm{MiB} = 128\\,\\mathrm{MiB}$。\n根据所提供的流式模型，每轮的 TLB 未命中次数等于遍历的不同页面的数量。\n基本页面大小为 $P_{base} = 4\\,\\mathrm{KiB}$。\n我们有 $1\\,\\mathrm{MiB} = 2^{20}$ 字节和 $1\\,\\mathrm{KiB} = 2^{10}$ 字节。\n未命中次数计算如下：\n$$ \\text{Misses}_{\\text{default}} = \\frac{W}{P_{base}} = \\frac{128\\,\\mathrm{MiB}}{4\\,\\mathrm{KiB}} = \\frac{128 \\times 2^{20}\\,\\text{字节}}{4 \\times 2^{10}\\,\\text{字节}} = \\frac{2^7 \\times 2^{20}}{2^2 \\times 2^{10}} = 2^{7+20-2-10} = 2^{15} = 32768 $$\n因此，使用默认分配器，每轮有 $32768$ 次 TLB 未命中。\n\n**第二部分：对齐感知分配器的设计与分析**\n\n为了启用 THP，分配器必须满足指定的条件。由于每个数组的大小为 $S=1\\,\\mathrm{MiB}$，我们可以将它们成对组合以填充一个 $2\\,\\mathrm{MiB}$ 的区域，这正是大页的大小。一个稳健的分配器设计方案如下：\n1.  对于每对 $1\\,\\mathrm{MiB}$ 数组，分配一个大于 $2\\,\\mathrm{MiB}$ 的虚拟内存区域，以确保在其内部能找到一个 $2\\,\\mathrm{MiB}$ 对齐的地址。例如，通过单个 `mmap` 调用分配一个 $4\\,\\mathrm{MiB}$ 的块会创建一个单一的 VMA。\n2.  在这个较大的区域内，计算出第一个与 $2\\,\\mathrm{MiB}$ 边界对齐的虚拟地址。\n3.  从分配的起始位置到这个对齐地址之间的内存是“前导冗余空间”，而所需 $2\\,\\mathrm{MiB}$ 块末端之后的内存是“尾随冗余空间”。可以使用 `munmap` 释放这些冗余区域，以避免浪费内存。\n4.  这个过程会产生一个与 $2\\,\\mathrm{MiB}$ 边界对齐的 $2\\,\\mathrm{MiB}$ 大小的 VMA。\n5.  然后，分配器可以从此块中为两个数组返回两个 $1\\,\\mathrm{MiB}$ 的指针。\n\n当程序顺序扫描这两个数组时，这个 $2\\,\\mathrm{MiB}$ 对齐区域内所有组成的 $4\\,\\mathrm{KiB}$ 基本页面都将被置入并填充。由于现在所有条件（单个 VMA、$2\\,\\mathrm{MiB}$ 对齐、完全填充）都已满足，操作系统的 THP 机制可以成功地将该区域提升为单个 $2\\,\\mathrm{MiB}$ 的大页。\n\n采用这种新的分配策略后，整个 $W = 128\\,\\mathrm{MiB}$ 的工作集都由大页映射。大页的大小为 $P_{huge} = 2\\,\\mathrm{MiB}$。\n现在每轮的 TLB 未命中次数为：\n$$ \\text{Misses}_{\\text{aligned}} = \\frac{W}{P_{huge}} = \\frac{128\\,\\mathrm{MiB}}{2\\,\\mathrm{MiB}} = \\frac{128 \\times 2^{20}\\,\\text{字节}}{2 \\times 2^{20}\\,\\text{字节}} = \\frac{128}{2} = 64 $$\n切换到对齐感知的分配器将 TLB 未命中次数从 $32768$ 减少到 $64$。\n\n**逐项分析**\n\nA. **未对齐阻止了 THP，因为大页需要在单个 VMA 内有 $2\\,\\mathrm{MiB}$ 对齐的区域。设计一个分配器，通过系统调用（例如，请求 $4\\,\\mathrm{MiB}$）预留至少 $2\\,\\mathrm{MiB}$ 外加一些冗余空间，计算该预留空间内下一个 $2\\,\\mathrm{MiB}$ 对齐的基地址，通过取消映射来裁剪掉任何前导和尾随的冗余空间，然后从此 $2\\,\\mathrm{MiB}$ 对齐的超级块中为两个 $1\\,\\mathrm{MiB}$ 的数组进行子分配，并可选择使用建议性调用来倾向于使用 THP。对于 $W = 128\\,\\mathrm{MiB}$ 的顺序扫描，TLB 未命中次数从 $W / 4\\,\\mathrm{KiB} = 32768$ 下降到 $W / 2\\,\\mathrm{MiB} = 64$。**\n此选项正确地指出了使用默认分配器时 THP 失败的原因。所提出的分配器设计是合理、实用的，并直接解决了 THP 提升的要求。定量分析也是正确的，计算出初始未命中次数为 $32768$ 次，最终未命中次数为 $64$ 次。\n**结论：正确。**\n\nB. **将分配对齐到 $64\\,\\mathrm{KiB}$ 对于 THP 来说是足够的，因为硬件可以映射子大页单元。确保 $64\\,\\mathrm{KiB}$ 对齐的 malloc 将 TLB 未命中次数从 $W / 4\\,\\mathrm{KiB} = 32768$ 减少到 $W / 64\\,\\mathrm{KiB} = 2048$。**\n此选项不正确。问题明确指出大页大小为 $2\\,\\mathrm{MiB}$，且需要 $2\\,\\mathrm{MiB}$ 对齐。$64\\,\\mathrm{KiB}$ 的对齐是不够的。硬件可以映射任意“子大页单元”的前提不受问题陈述或典型的 x86-64 架构支持，后者具有固定的页面大小（$4\\,\\mathrm{KiB}$、$2\\,\\mathrm{MiB}$、$1\\,\\mathrm{GiB}$）。最终未命中次数的计算基于一个不存在的 $64\\,\\mathrm{KiB}$ 页面大小。\n**结论：不正确。**\n\nC. **使用标准的对齐分配调用将每个 $1\\,\\mathrm{MiB}$ 数组放置在 $2\\,\\mathrm{MiB}$ 边界上，即使每个数组都在其自己的映射中，也足以实现 THP 提升，因此 TLB 未命中次数从 $32768$ 减少到 $W / 1\\,\\mathrm{MiB} = 128$。**\n此选项不正确，原因有二。首先，它错误地声称 THP 可以跨越独立的映射（VMA）工作，这与一个给定的条件相矛盾。其次，将单个 $1\\,\\mathrm{MiB}$ 数组放置在一个 $2\\,\\mathrm{MiB}$ 对齐的区域中并不能保证整个 $2\\,\\mathrm{MiB}$ 区域都会被填充，这是 THP 的另一个条件。定量预测使用了一个虚构的 $1\\,\\mathrm{MiB}$ 页面大小。\n**结论：不正确。**\n\nD. **当内核的后台合并器运行时，THP 提升不受对齐影响，因此正确的设计是保持 malloc 不变；每轮的 TLB 未命中次数仍为 $32768$ 次，因为在这种情况下，是 TLB 容量而不是对齐决定了未命中次数。**\n此选项从根本上是错误的。对齐是 THP 的一个严格且不可协商的先决条件。内核的合并守护进程（`khugepaged`）无法在分配后修复对齐问题；它只能提升那些已经正确对齐和结构化的区域。对齐无关紧要的断言是错误的。\n**结论：不正确。**\n\nE. **为避免碎片化，通过建议性调用禁用 THP 并保留 $4\\,\\mathrm{KiB}$ 页面；这会改善局部性，并将 TLB 未命中次数减少到 $W / 8\\,\\mathrm{KiB} = 16384$，因为在没有大页的情况下实现了更大的查找粒度。**\n此选项不合逻辑。禁用 THP 会强制使用 $4\\,\\mathrm{KiB}$ 页面，这与现状相同。这样做不可能减少 TLB 未命中次数。声称这能实现 $8\\,\\mathrm{KiB}$ 的“更大查找粒度”是自相矛盾且在物理上是荒谬的。页面大小将保持为 $4\\,\\mathrm{KiB}$。\n**结论：不正确。**", "answer": "$$\\boxed{A}$$", "id": "3684889"}, {"introduction": "启用大页并非总能带来正面效果，其性能影响与应用程序的内存访问模式密切相关。本实践模拟了一个性能调优实验，通过分析一个内存密集型基准测试在不同透明大页（THP）策略（`always`、`never`、`madvise`）下的吞吐量和延迟数据，让你亲身体会顺序访问与随机访问模式下大页带来的不同效果。通过这个练习，你将学会如何在减少TLB未命中的巨大优势与后台内存规整（compaction）可能引入的延迟抖动之间做出权衡 [@problem_id:3684922]。", "problem": "一个运行 Linux 操作系统 (OS) 的系统支持透明大页 (Transparent Huge Pages, THP)，可通过位于 `/sys/kernel/mm/transparent_hugepage/enabled` 的 sysfs 接口进行配置，该接口有三种模式：`always`、`never` 和 `madvise`。THP 会在可能的情况下，将大小为 $p=4\\,\\mathrm{KiB}$ 的常规基页提升为大小为 $P=2\\,\\mathrm{MiB}$ 的大页（也称为超级页）。中央处理器 (CPU) 依赖转译后备缓冲器 (Translation Lookaside Buffer, TLB) 来缓存虚拟到物理地址的转换；使用更大的页面可以减少覆盖一个工作集所需的不同页面转换的数量。大页大小与基页大小之比 $r$ 为 $r = \\dfrac{P}{p} = \\dfrac{2\\,\\mathrm{MiB}}{4\\,\\mathrm{KiB}} = 512$，因此每个大页覆盖 512 个基页。\n\n要求您设计并分析一个实验，该实验调整 THP 并测量一个内存密集型基准测试的吞吐量 $T$ 和延迟 $L$。该基准测试有两个阶段，运行在一台具有足够随机存取存储器 (RAM) 的机器上，因此不会发生交换。\n\n- 阶段 1 (顺序流式访问)：基准测试分配一个大小为 $W=8\\,\\mathrm{GiB}$ 的工作集匿名内存，并在整个区域上以 $s=64\\,\\mathrm{B}$ 的步长执行顺序读取。吞吐量 $T$ 以 $\\mathrm{GiB/s}$ 为单位进行测量，每次访问的延迟中位数 $L_{50}$ 以 $\\mathrm{ns}$ 为单位进行测量。在 `madvise` 模式下，基准测试对分配的区域调用带有 `MADV_HUGEPAGE` 的建议性系统调用 `madvise`；否则，不调用。测量结果：\n  - `always`: $T_{\\mathrm{always}} = 12.0\\,\\mathrm{GiB/s}$, $L_{50,\\mathrm{always}} = 140\\,\\mathrm{ns}$\n  - `madvise`: $T_{\\mathrm{madvise}} = 11.3\\,\\mathrm{GiB/s}$, $L_{50,\\mathrm{madvise}} = 160\\,\\mathrm{ns}$\n  - `never`: $T_{\\mathrm{never}} = 9.4\\,\\mathrm{GiB/s}$, $L_{50,\\mathrm{never}} = 210\\,\\mathrm{ns}$\n\n- 阶段 2 (随机指针追逐)：基准测试分配相同大小的工作集 $W=8\\,\\mathrm{GiB}$，并使用 $n=32$ 个线程执行指针追逐；每个操作解引用一个随机分布在 $W$ 区域内的指针。吞吐量 $T$ 以 $\\mathrm{ops/s}$ 为单位进行测量，第 $99$ 百分位延迟 $L_{99}$ 以 $\\mathrm{ms}$ 为单位进行测量。在 `madvise` 模式下，基准测试再次使用 `MADV_HUGEPAGE`。测量结果：\n  - `always`: $T_{\\mathrm{always}} = 3.0 \\times 10^{6}\\,\\mathrm{ops/s}$, $L_{99,\\mathrm{always}} = 4.2\\,\\mathrm{ms}$\n  - `madvise`: $T_{\\mathrm{madvise}} = 3.1 \\times 10^{6}\\,\\mathrm{ops/s}$, $L_{99,\\mathrm{madvise}} = 2.6\\,\\mathrm{ms}$\n  - `never`: $T_{\\mathrm{never}} = 3.0 \\times 10^{6}\\,\\mathrm{ops/s}$, $L_{99,\\mathrm{never}} = 2.4\\,\\mathrm{ms}$\n\n假设有一个容量有限的常规 TLB，可用于基页和大页（具体数量未给出），并且 `always` 模式下的透明大页 (THP) 会尝试后台内存规整，通过 `khugepaged` 和 `kcompactd` 等内核线程为大页创建物理上连续的内存，这可能会引入偶尔的停顿。交换和 I/O 不是影响因素。\n\n基于分页的基本定义和 TLB 的作用，推断大页在顺序和随机访问下对 TLB 未命中的预期影响，以及 THP 内存规整的潜在开销。哪项策略建议与测量结果和第一性原理分析最吻合，以便为该基准测试最大化吞吐量 $T$ 同时保持低尾部延迟 $L$？\n\nA. 对所有工作负载使用 `always` 模式，因为无论访问模式如何，大页总能通过减少页错误和 TLB 未命中来提高吞吐量和降低延迟。\n\nB. 对顺序流式访问使用 `madvise` 模式 (带有 `MADV_HUGEPAGE`)，对随机指针追逐使用 `never` 模式；大页在流式访问中减少了 TLB 未命中和页表遍历，但 `always` 模式可能引入后台内存规整停顿，这会增加随机工作负载中的 $L_{99}$。\n\nC. 对所有工作负载使用 `never` 模式，因为大页增加了缓存污染，因此普遍降低了吞吐量。\n\nD. 仅在 RAM 处于内存压力下时使用 `always` 模式，因为 THP 内存规整主要有利于交换行为，因此会降低随机指针追逐中的延迟。", "solution": "问题的核心在于理解使用大页与基页之间的性能权衡，而这种权衡受到不同 THP 策略的调节。大页的主要好处是减少 TLB 压力。一个用于 $2\\,\\mathrm{MiB}$ 大页的 TLB 条目覆盖的内存范围与 512 个用于 $4\\,\\mathrm{KiB}$ 基页的条目相同。TLB 未命中的代价很高，需要硬件进行页表遍历，这可能需要数百个 CPU 周期。THP 的主要成本，尤其是在 `always` 模式下，是创建大页的开销，这需要物理上连续的内存。内核可能需要执行内存规整，这涉及移动现有数据，并可能导致显著的、不可预测的应用程序停顿。\n\n#### 阶段 1 分析：顺序流式访问\n\n1.  **第一性原理**：顺序流式访问模式表现出完美的空间局部性。应用程序将以线性、可预测的顺序访问内存地址。这是大页的理想用例。一旦大页的 TLB 条目被缓存，就可以访问大片内存区域 ($2\\,\\mathrm{MiB}$) 而无需进一步的 TLB 未命中。使用基页将导致每隔几个页面（取决于 TLB 覆盖范围）就发生一次 TLB 未命中，从而产生重复的页表遍历惩罚。因此，我们预期大页（`always` 和 `madvise` 模式）的性能会显著优于 `never` 模式。\n2.  **数据分析**：\n    -   `never`: $T_{\\mathrm{never}} = 9.4\\,\\mathrm{GiB/s}$, $L_{50,\\mathrm{never}} = 210\\,\\mathrm{ns}$。这是低性能基线，受限于频繁的 TLB 未命中。\n    -   `madvise`: $T_{\\mathrm{madvise}} = 11.3\\,\\mathrm{GiB/s}$ (比 `never` 提升 $20\\%$), $L_{50,\\mathrm{madvise}} = 160\\,\\mathrm{ns}$。应用程序的提示使内核能够使用大页，从而大幅减少 TLB 未命中，提高了吞吐量和中位数延迟。\n    -   `always`: $T_{\\mathrm{always}} = 12.0\\,\\mathrm{GiB/s}$ (比 `never` 提升 $28\\%$), $L_{50,\\mathrm{always}} = 140\\,\\mathrm{ns}$。对于这种完全适合的工作负载，内核为创建大页所做的前瞻性工作甚至更有效，从而带来了最佳性能。任何后台规整的开销与页表遍历延迟的大幅减少相比都微不足道，并且没有反映在中位数延迟中。\n\n**阶段 1 结论**：大页无疑是有益的。`always` 模式是最佳的，`madvise` 是一个非常强的次优选择。\n\n#### 阶段 2 分析：随机指针追逐\n\n1.  **第一性原理**：在大工作集 ($W=8\\,\\mathrm{GiB}$) 上的随机访问模式空间局部性差。然而，覆盖这个工作集所需的独立页面数量非常大：$W/p = (8 \\times 2^{30}) / (4 \\times 2^{10}) = 2,097,152$ 个基页。没有 TLB 能缓存这么多的转换。使用大页将转换数量减少到 $W/P = (8 \\times 2^{30}) / (2 \\times 2^{20}) = 4096$ 个大页。这个数量足够小，现代 TLB 可以缓存其中很大一部分，因此原则上，与基页相比，大页仍应能降低 TLB 未命中率。然而，`always` 模式的后台内存规整活动可能引入不可预测的停顿。这些停顿——尽管不频繁——可能非常长，严重影响像第 99 百分位 ($L_{99}$) 这样的尾部延迟指标。对于延迟敏感型工作负载，这是一个主要问题。`madvise` 模式不那么激进，可以避免一些这种病态的尾部延迟，而 `never` 则完全避免了它。\n2.  **数据分析**：\n    -   `never`: $T_{\\mathrm{never}} = 3.0 \\times 10^{6}\\,\\mathrm{ops/s}$, $L_{99,\\mathrm{never}} = 2.4\\,\\mathrm{ms}$。这是基线。性能是可预测的，尾部延迟最低。\n    -   `madvise`: $T_{\\mathrm{madvise}} = 3.1 \\times 10^{6}\\,\\mathrm{ops/s}$ (吞吐量有边际提升), $L_{99,\\mathrm{madvise}} = 2.6\\,\\mathrm{ms}$ (尾部延迟有边际增加)。这反映了权衡：TLB 未命中的轻微减少提高了总吞吐量，但创建大页的开销（即使是按需创建）增加了少量的尾部延迟。\n    -   `always`: $T_{\\mathrm{always}} = 3.0 \\times 10^{6}\\,\\mathrm{ops/s}$ (吞吐量无提升), $L_{99,\\mathrm{always}} = 4.2\\,\\mathrm{ms}$ (尾部延迟比 `never` 增加了 $75\\%$)。这是关键结果。主动的、系统范围的内存规整停顿对尾部延迟是毁灭性的。在第 99 百分位上，减少 TLB 未命中的好处被内核引起的停顿完全抵消了。\n\n**阶段 2 结论**：`always` 模式因其极高的尾部延迟而不可接受。`never` 模式提供了最佳的可预测性和最低的尾部延迟。`madvise` 以略差的尾部延迟为代价提供了微不足道的吞吐量增益，使得 `never` 成为这个延迟敏感阶段的更佳选择。\n\n#### 综合与策略建议\n\n目标是在保持低尾部延迟的同时最大化吞吐量。必须根据工作负载选择最优策略。\n-   对于**顺序流式访问**，大页对于高吞吐量至关重要。`madvise` 或 `always` 是不错的选择。\n-   对于**随机指针追逐**，其中低尾部延迟是优先考虑的，`always` 是有害的。`never` 在延迟指标上是最安全和性能最好的选择。\n\n因此，一个混合策略是可行的：对受益的工作负载使用大页，对受损的工作负载禁用它们。这正是 `madvise` 设置的设计初衷——应用程序引导的优化。然而，问题要求的是一个可能涉及设置系统级开关的策略建议。从测量结果来看，最佳组合是在阶段 1 使用大页（例如，通过 `madvise`），在阶段 2 不使用大页 (`never`)。\n\n### 逐项选项分析\n\n**A. 对所有工作负载使用 `always` 模式，因为无论访问模式如何，大页总能通过减少页错误和 TLB 未命中来提高吞吐量和降低延迟。**\n这种说法实际上是错误的。阶段 2 的数据清楚地表明，`always` 模式显著恶化了尾部延迟 ($L_{99}$)，从 $2.4\\,\\mathrm{ms}$ 增加到 $4.2\\,\\mathrm{ms}$，而没有提高吞吐量。大页“总能提高”性能的前提是一个错误的过度概括。\n**结论：不正确。**\n\n**B. 对顺序流式访问使用 `madvise` 模式 (带有 `MADV_HUGEPAGE`)，对随机指针追逐使用 `never` 模式；大页在流式访问中减少了 TLB 未命中和页表遍历，但 `always` 模式可能引入后台内存规整停顿，这会增加随机工作负载中的 $L_{99}$。**\n该选项准确地反映了从数据和第一性原理推导出的最优策略。\n-   它正确地指出大页对流式访问有益。为此阶段使用 `madvise` 带来了高吞吐量 ($11.3\\,\\mathrm{GiB/s}$)，接近最佳情况的 `always` 模式。\n-   它正确地指出对于随机工作负载，`never` 模式在保持低尾部延迟 ($L_{99} = 2.4\\,\\mathrm{ms}$) 方面更优越。\n-   提供的理由是合理的：大页在流式访问中减少 TLB 未命中，而 `always` 模式的内存规整停顿损害了随机工作负载的尾部延迟。这与我们的分析完全吻合。\n**结论：正确。**\n\n**C. 对所有工作负载使用 `never` 模式，因为大页增加了缓存污染，因此普遍降低了吞吐量。**\n这种说法与阶段 1 的数据相矛盾，在该阶段，`always` 和 `madvise` 模式的吞吐量都远高于 `never` 模式 ($12.0\\,\\mathrm{GiB/s}$ 和 $11.3\\,\\mathrm{GiB/s}$ vs. $9.4\\,\\mathrm{GiB/s}$)。关于普遍降低吞吐量的说法是错误的。\n**结论：不正确。**\n\n**D. 仅在 RAM 处于内存压力下时使用 `always` 模式，因为 THP 内存规整主要有利于交换行为，因此会降低随机指针追逐中的延迟。**\n这种说法在多个方面都是不正确的。\n-   问题明确指出没有内存压力或交换。\n-   THP 内存规整的主要目的是为大页创建连续的物理内存，而不是为了有利于交换。\n-   关于 `always` 模式会“降低随机指针追逐中的延迟”的说法被实验数据直接驳斥，数据显示它显著增加了尾部延迟。\n**结论：不正确。**", "answer": "$$\\boxed{B}$$", "id": "3684922"}, {"introduction": "除了对性能延迟的影响，采用大页还可能带来内存使用效率方面的问题，尤其是在处理可变大小对象时。本实践将探讨一个常见的场景：一个键值存储系统使用固定大小的内存池（arenas）来管理内存，而这些池本身则由大页支持。通过对一个假设的值大小分布进行分析，你将学习如何从第一性原理出发，定量计算因向上取整和内存池末端空间浪费而导致的内部碎片，从而深刻理解在追求极致性能时必须付出的内存效率代价 [@problem_id:3684843]。", "problem": "一个键值存储运行在一个支持大页的操作系统上，它将分配器区域固定到大小为 $2\\,\\mathrm{MiB}$ 的大页上（每个区域一个大页）。每个区域专用于一个大小类，并为该类子分配为大小为 $s$ 字节的固定大小的槽。槽是对齐且不重叠的；如果 $k$ 个大小为 $s$ 的槽能放入一个 $2\\,\\mathrm{MiB}$ 的大页中，该页中任何剩余的字节都将不被使用，并且不能被其他大小类借用，因为区域被固定到了一个类上。分配器将每个请求的值大小 $v$ 向上取整到满足 $s \\ge v$ 的最小槽大小 $s$。忽略元数据和外部碎片；假设有大量分配，因此每个类的区域都密集地填充了 $k$ 个存活对象，只留下页尾的剩余空间未使用。\n\n假设分配器使用大小类集合 $S=\\{1\\,\\mathrm{KiB},\\,1.5\\,\\mathrm{KiB},\\,3\\,\\mathrm{KiB},\\,8\\,\\mathrm{KiB},\\,48\\,\\mathrm{KiB},\\,256\\,\\mathrm{KiB},\\,1\\,\\mathrm{MiB}\\}$ 并将请求 $v$ 映射到 $s(v)=\\min\\{s \\in S \\mid s \\ge v\\}$。因此，单个请求 $v$ 消耗的摊销提交内存等于其所在类的区域中每个对象的份额，即大页大小除以能容纳于该大页的完整槽数 $k$。\n\n设值大小 $V$（单位为字节）具有以下离散分布 $D(v)$：\n- $\\Pr[V=600]=0.25$,\n- $\\Pr[V=1100]=0.20$,\n- $\\Pr[V=2900]=0.15$,\n- $\\Pr[V=7500]=0.15$,\n- $\\Pr[V=40000]=0.10$,\n- $\\Pr[V=260000]=0.10$,\n- $\\Pr[V=900000]=0.05$.\n\n仅使用分页、取整和期望的基本定义，从基本原理推导出期望内部碎片率，其定义为\n$$F \\equiv \\frac{\\text{期望提交字节数} - \\text{期望使用字节数}}{\\text{期望提交字节数}},$$\n在上述区域固定策略下。取 $1\\,\\mathrm{MiB}=2^{20}\\,\\text{字节}$ 和 $1\\,\\mathrm{KiB}=1024\\,\\text{字节}$。将最终结果表示为一个无单位的十进制数，并将您的答案四舍五入到四位有效数字。", "solution": "我们使用的基本定义是：\n- 一个大页的大小为 $H = 2\\,\\mathrm{MiB} = 2 \\times 2^{20} = 2{,}097{,}152$ 字节。\n- 对于槽大小为 $s$ 的大小类，一个大页中的完整槽数为 $k=\\left\\lfloor \\frac{H}{s} \\right\\rfloor$，每个对象的提交份额为 $\\frac{H}{k}$ 字节，因为 $k$ 个对象平均共享 $H$ 字节。\n- 分配器将 $v$ 向上取整到 $s(v) \\in S$，其中 $s(v) \\ge v$ 且 $s(v)$ 在 $S$ 中是最小的。\n- 期望内部碎片率为\n$$F \\equiv \\frac{\\mathbb{E}[\\text{每个对象的提交内存}] - \\mathbb{E}[V]}{\\mathbb{E}[\\text{每个对象的提交内存}]} = 1 - \\frac{\\mathbb{E}[V]}{\\mathbb{E}[\\text{每个对象的提交内存}]}.$$\n\n步骤 1：将每个值 $v$ 映射到其槽大小 $s(v)$，并计算 $k(v)=\\left\\lfloor \\frac{H}{s(v)} \\right\\rfloor$ 和每个对象的提交份额 $C(v)=\\frac{H}{k(v)}$。\n\n我们有 $H=2{,}097{,}152$。\n\n- 对于 $v=600$，$s(v)=1\\,\\mathrm{KiB}=1{,}024$。那么 $k=\\left\\lfloor \\frac{2{,}097{,}152}{1{,}024} \\right\\rfloor = 2{,}048$ 且 $C(v)=\\frac{2{,}097{,}152}{2{,}048}=1{,}024$。\n- 对于 $v=1100$，$s(v)=1.5\\,\\mathrm{KiB}=1{,}536$。那么 $k=\\left\\lfloor \\frac{2{,}097{,}152}{1{,}536} \\right\\rfloor = \\left\\lfloor \\frac{2{,}097{,}152/512}{3} \\right\\rfloor = \\left\\lfloor \\frac{4{,}096}{3} \\right\\rfloor = 1{,}365$，且 $C(v)=\\frac{2{,}097{,}152}{1{,}365} = 1{,}536 + \\frac{512}{1{,}365} = 1{,}536.375091575\\ldots$。\n- 对于 $v=2900$，$s(v)=3\\,\\mathrm{KiB}=3{,}072$。那么 $k=\\left\\lfloor \\frac{2{,}097{,}152}{3{,}072} \\right\\rfloor = \\left\\lfloor \\frac{2{,}097{,}152/1{,}024}{3} \\right\\rfloor = \\left\\lfloor \\frac{2{,}048}{3} \\right\\rfloor = 682$，且 $C(v)=\\frac{2{,}097{,}152}{682} = 3{,}075 + \\frac{2}{682} = 3{,}075.002932551\\ldots$。\n- 对于 $v=7{,}500$，$s(v)=8\\,\\mathrm{KiB}=8{,}192$。那么 $k=\\frac{2{,}097{,}152}{8{,}192}=256$（恰好），且 $C(v)=8{,}192$。\n- 对于 $v=40{,}000$，$s(v)=48\\,\\mathrm{KiB}=49{,}152$。那么 $k=\\left\\lfloor \\frac{2{,}097{,}152}{49{,}152} \\right\\rfloor = \\left\\lfloor \\frac{2{,}048}{48} \\right\\rfloor = 42$，且 $C(v)=\\frac{2{,}097{,}152}{42} = \\frac{1{,}048{,}576}{21} = 49{,}932.19047619\\ldots$。\n- 对于 $v=260{,}000$，$s(v)=256\\,\\mathrm{KiB}=262{,}144$。那么 $k=\\frac{2{,}097{,}152}{262{,}144}=8$（恰好），且 $C(v)=262{,}144$。\n- 对于 $v=900{,}000$，$s(v)=1\\,\\mathrm{MiB}=1{,}048{,}576$。那么 $k=\\frac{2{,}097{,}152}{1{,}048{,}576}=2$（恰好），且 $C(v)=1{,}048{,}576$。\n\n步骤 2：根据给定的分布计算 $\\mathbb{E}[V]$。\n\n$$\n\\mathbb{E}[V] = 0.25 \\cdot 600 + 0.20 \\cdot 1{,}100 + 0.15 \\cdot 2{,}900 + 0.15 \\cdot 7{,}500 + 0.10 \\cdot 40{,}000 + 0.10 \\cdot 260{,}000 + 0.05 \\cdot 900{,}000.\n$$\n\n逐项计算：\n\n$$\n\\mathbb{E}[V] = 150 + 220 + 435 + 1{,}125 + 4{,}000 + 26{,}000 + 45{,}000 = 76{,}930.\n$$\n\n\n步骤 3：通过用相同的概率对 $C(v)$ 进行加权，计算每个对象的提交内存的期望值 $\\mathbb{E}[\\text{committed per object}] = \\mathbb{E}[C(V)]$。\n\n$$\n\\mathbb{E}[C(V)] = 0.25 \\cdot 1{,}024 + 0.20 \\cdot \\frac{2{,}097{,}152}{1{,}365} + 0.15 \\cdot \\frac{2{,}097{,}152}{682} + 0.15 \\cdot 8{,}192 + 0.10 \\cdot \\frac{2{,}097{,}152}{42} + 0.10 \\cdot 262{,}144 + 0.05 \\cdot 1{,}048{,}576.\n$$\n\n计算每一项：\n- $0.25 \\cdot 1{,}024 = 256$。\n- $0.20 \\cdot \\frac{2{,}097{,}152}{1{,}365} = \\frac{2{,}097{,}152}{6{,}825} \\approx 307.27501832$。\n- $0.15 \\cdot \\frac{2{,}097{,}152}{682} = \\frac{6{,}291{,}456}{13{,}640} \\approx 461.25043978$。\n- $0.15 \\cdot 8{,}192 = 1{,}228.8$。\n- $0.10 \\cdot \\frac{2{,}097{,}152}{42} = \\frac{2{,}097{,}152}{420} \\approx 4{,}993.21904762$。\n- $0.10 \\cdot 262{,}144 = 26{,}214.4$。\n- $0.05 \\cdot 1{,}048{,}576 = 52{,}428.8$。\n\n求和，\n\n$$\n\\mathbb{E}[C(V)] \\approx 256 + 307.27501832 + 461.25043978 + 1{,}228.8 + 4{,}993.21904762 + 26{,}214.4 + 52{,}428.8 \\approx 85{,}889.74450572.\n$$\n\n\n步骤 4：计算期望内部碎片率\n\n$$\nF = 1 - \\frac{\\mathbb{E}[V]}{\\mathbb{E}[C(V)]} = 1 - \\frac{76{,}930}{85{,}889.74450572}.\n$$\n\n计算比率：\n\n$$\n\\frac{76{,}930}{85{,}889.74450572} \\approx 0.8956832,\n$$\n\n所以\n\n$$\nF \\approx 1 - 0.8956832 = 0.1043168.\n$$\n\n\n按要求四舍五入到四位有效数字，得到 $0.1043$。", "answer": "$$\\boxed{0.1043}$$", "id": "3684843"}]}